[#]: subject: "14 Top Outstanding Open Source LLMs For Research and Commercial Use"
[#]: via: "https://itsfoss.com/open-source-llms/"
[#]: author: "Ankush Das https://itsfoss.com/author/ankush/"
[#]: collector: "lujun9972/lctt-scripts-1705972010"
[#]: translator: " "
[#]: reviewer: " "
[#]: publisher: " "
[#]: url: " "

14 Top Outstanding Open Source LLMs For Research and Commercial Use
======

[![Warp Terminal][1]][2]

Large Language Models (or LLMs) are machine learning models ( _built on top of transformer neural network_ ) that aim to solve language problems like auto-completion, text classification, text generation, and more. It is a subset of Deep Learning.

The "large" refers to the model being pre-trained by a massive set of data (could be in sizes of petabytes), accompanied by millions or billions of parameters and further fine-tuned for a specific use-case.

While LLMs made [projects like ChatGPT][3] and Google Gemini possible, they are proprietary. So, there's a lack of transparency, and the entire control of the LLM (and its usage) is at the discretion of the company that owns it.

So, what's the solution to this? **Open source LLMs**.

With an open-source LLM, you can have multiple benefits like **transparency, no vendor lock-in, free to use for commercial purposes, and total control over customization of the model (in terms of performance, carbon footprint, and more)**.

Similar reasons why we need open-source ChatGPT chatbot alternatives:

![][4]

📋

And as promoters of open-source, we're not alone. IBM, Meta, Intel, AMD, CERN, Hugging Face, and more companies agree with the open innovation for AI idea and have come up with an [AI Alliance in 2023][5].

Now that you know why we need open-source LLMs, let me highlight some of the best ones available.

### 1\. Falcon 180B

The Technology Innovation Institute (TII) in the United Arab Emirates (UAE) launched an open LLM, which performs close to its proprietary competitors.

The model includes 180 billion parameters and was trained on 3.5 trillion tokens.

In the Falcon AI model family, there are smaller/newer models like Falcon 2 11B, which is a scalable solution, and outperforms similar models from Meta and Google.

💡

• ****Parameters**** : 180 billion
• ****License**** : [Falcon-180B TII License][6] (based on Apache 2.0)
• ****Usage**** : Allowed for commercial use and should be fine-tuned for specific tasks

### 2\. Dolly 2.0

Dolly 2.0 is one of the most prominent open-source LLM models developed by Databricks, fine-tuned on a human-generated instruction dataset. Unlike its predecessor, the dataset is its original, generated by more than 5000 employees of the company.

It includes 12 billion parameters, and is geared towards summarization, QA, classification, and some brainstorming. Dolly 2.0 does not aim to compete with existing models, but the focus was more on providing a human-generated dataset along with an LLM. They claimed it to be the first of its kind back then that tried to mimic ChatGPT's interaction.

💡

• ****Parameters**** : 12 billion
• ****License**** : MIT
• ****Usage**** : Allowed for research and commercial use

[Dolly 12 B][7]

📋

Many open-source LLMs offer variations of the models, fine-tuned for a specific task. You must check the license for them; it may not be the same as the one listed here.

### 3\. Cerebras-GPT

![][8]

Cerebras-GPT is a family of seven GPT-3 open-source LLM models ranging from 111 million to 13 billion parameters.

These models aim for higher accuracy with a faster training times, lower cost, and consume less energy. It is trained using DeepMind's Chinchilla formula.

💡

• ****Parameters**** : 12 billion
• ****License**** : Apache 2.0
• ****Usage**** : Allowed for research and commercial use and fine-tuned for ChatGPT like interaction

[Cerebrus GPT 13-B][9]

### 4\. Bloom

Bloom is an open-access multilingual LLM model that is trained to continue text from prompts, and can be trained for other text-based tasks.

The model includes 176 billion parameters, and can output in 46 different languages and 13 programming languages.

💡

• ****Parameters**** : 176 billion
• ****License**** : RAIL License v1.0
• ****Usage**** : Allowed for research and non-commercial entities

[Bloom][10]

### 5\. DLite V2

![][11]

DLite by AI Squared aims to present lightweight LLMs that you can use almost anywhere.

The family of model includes variations ranging from 123 million to 1.5 billion parameters. It utilizes the same database built by Databricks for Dolly 2.0. Hence, it is tuned for the same use-case.

💡

• ****Parameters**** : 1.5 billion
• ****License**** : Apache 2.0
• ****Usage**** : Allowed for research and commercial use and fine-tuned for ChatGPT like interaction

[DLite v2][12]

### 6\. MPT-7B

![][13]

MPT-7B is another prominent open-source model by [Databricks][14]. Unlike Dolly 2.0, it competes and surpasses Meta's LLaMA-7B in many ways.

The model was trained using a mixed set of data from various sources, including 6.7 billion parameters. It also aims to be an affordable alternative to the closed source LLM models available.

💡

• ****Parameters**** : 6.7 billion
• ****License**** : Apache 2.0
• ****Usage**** : Allowed for research and commercial use and fine-tuned for ChatGPT like interaction

[MPT-7B][15]

### 7\. XGen 7B

![][16]

XGen 7B is an LLM model by [Salesforce][17], one of the leading companies in its industry. The model has been trained for text generation and code tasks, claiming performance benefits over similar models like MPT.

💡

• ****Parameters**** : 6.7 billion
• ****License**** : Apache 2.0
• ****Usage**** : Allowed for research and commercial use and fine-tuned for text generation/code tasks

[XGen 7B][18]

### 8\. OpenLLaMA

![][19]

OpenLLaMA is a reproduction of Meta's popular LLaMa model trained with a different dataset to make it available under a permissive license, when compared to Meta's custom commercial license.

Currently, you can find OpenLLaMA 7Bv2 and a 3Bv3 model, with all of them trained using a mixture of datasets.

You can learn more about it on its GitHub page.

💡

• ****Parameters**** : 7 billion
• ****License**** : Apache 2.0
• ****Usage**** : Allowed for research and commercial use

[OpenLLaMA][20]

### 9\. StarCoder

![][21]

StarCoder is an interesting open-source LLM trained using data from GitHub (commits, issues etc) fine-tuned for coding tasks.

It includes 15 billion parameters and trained on 80+ programming languages. Considering the data is from GitHub, you will need to add attribution wherever necessary when you use the model.

💡

• ****Parameters**** : 15 billion
• ****License**** : [Open RAIL-M v1][22]
• ****Usage**** : Allowed for research and commercial use (with restrictions not to misuse it)

[StarCoder][23]

### 10\. CodeT5

![][24]

Another open-source from Salesforce, fine-tuned for coding tasks and code generation. CodeT5 is one of the most competing code LLMs that features 16 billion parameters, and claims to outperform OpenAI's code-cushman001 model.

💡

• ****Parameters**** : 16 billion
• ****License**** : BSD 3-Clause
• ****Usage**** : Allowed for research and non-commercial use

[CodeT5][25]

### 11\. Yi-1.5

![][26]

Yi-1.5 is an exciting open-source LLM featuring a whopping 34.4 billion parameters.

It is fine-tuned for coding, math, reasoning, and instruction-following capability. Originally, this model wasn't open-source, but later it was. And, that's a good thing.

💡

• ****Parameters**** : 34.4 billion
• ****License**** : Apache 2.0
• ****Usage**** : Allowed for research and commercial use

[Yi-1.5][27]

### 12\. SOLAR-10.7B

![][28]

SOLAR-10.7B is a pre-trained open-source LLM by Upstage.ai that just generates random text. You need to fine-tune it to adapt to your particular requirements.

When compared to models with larger parameters, it promises to provide a good model performance efficiency and scalability even with 10.7 billion parameters. It is an incredibly popular adaptable language model.

💡

• ****Parameters**** : 10.7 billion
• ****License**** : Apache 2.0
• ****Usage**** : Allowed for research and commercial use

[SOLAR-10.7B][29]

### 13\. Mistral 7B

![][30]

Mistral 7B is a powerful language model that claims to outperform LLaMa 1/2 in various ways. The large language model is capable of math, reasoning, word knowledge, QA, and more.

It is a pre-trained generative text model with 7 billion parameters. Hence, it does not have any moderation mechanisms. So, you need to fine-tune it to output moderated results.

💡

• ****Parameters**** : 7 billion
• ****License**** : Apache 2.0
• ****Usage**** : Allowed for research and commercial use

[Mistral 7B][31]

### 14\. OLMo-7B

![][32]

OLMo-7B is yet another open language model pre-trained with Dolma dataset featuring three trillion token open corpus. The entire training data, model weights, and the code is open-source, and available to use without restriction.

💡

• ****Parameters**** : 7 billion
• ****License**** : Apache 2.0
• ****Usage**** : Allowed for research and commercial use

[OLMo-7B][33]

_💬 What is your favorite open-source LLM? Have you tried running any of them? Share your thoughts in the comments below!_

--------------------------------------------------------------------------------

via: https://itsfoss.com/open-source-llms/

作者：[Ankush Das][a]
选题：[lujun9972][b]
译者：[译者ID](https://github.com/译者ID)
校对：[校对者ID](https://github.com/校对者ID)

本文由 [LCTT](https://github.com/LCTT/TranslateProject) 原创编译，[Linux中国](https://linux.cn/) 荣誉推出

[a]: https://itsfoss.com/author/ankush/
[b]: https://github.com/lujun9972
[1]: https://itsfoss.com/assets/images/warp-terminal.webp
[2]: https://www.warp.dev?utm_source=its_foss&utm_medium=display&utm_campaign=linux_launch
[3]: https://itsfoss.com/open-source-chatgpt-alternatives/
[4]: https://itsfoss.com/content/images/size/w256h256/2022/12/android-chrome-192x192.png
[5]: https://news.itsfoss.com/ai-alliance/
[6]: https://huggingface.co/tiiuae/falcon-180B/blob/main/LICENSE.txt
[7]: https://huggingface.co/databricks/dolly-v2-12b
[8]: https://itsfoss.com/content/images/2024/05/cerebras-stats-1.png
[9]: https://huggingface.co/cerebras/Cerebras-GPT-13B
[10]: https://huggingface.co/bigscience/bloom
[11]: https://itsfoss.com/content/images/2024/05/dlite.png
[12]: https://huggingface.co/aisquared/dlite-v2-1_5b
[13]: https://itsfoss.com/content/images/2024/05/mpt-7b.png
[14]: https://www.databricks.com/
[15]: https://huggingface.co/mosaicml/mpt-7b
[16]: https://itsfoss.com/content/images/2024/05/xgen7.png
[17]: https://www.salesforce.com/
[18]: https://github.com/salesforce/xgen
[19]: https://itsfoss.com/content/images/2024/05/openllma.png
[20]: https://github.com/openlm-research/open_llama
[21]: https://itsfoss.com/content/images/2024/05/starcoder.png
[22]: https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement
[23]: https://huggingface.co/bigcode/starcoder
[24]: https://itsfoss.com/content/images/2024/05/code-t5-.png
[25]: https://github.com/salesforce/CodeT5/tree/main/CodeT5%2B
[26]: https://itsfoss.com/content/images/2024/05/yi-1.5.png
[27]: https://github.com/01-ai/Yi-1.5
[28]: https://itsfoss.com/content/images/2024/05/solar-stats.png
[29]: https://huggingface.co/upstage/SOLAR-10.7B-v1.0
[30]: https://itsfoss.com/content/images/2024/05/mistral-7b.png
[31]: https://huggingface.co/mistralai/Mistral-7B-v0.1
[32]: https://itsfoss.com/content/images/2024/05/olmo.png
[33]: https://huggingface.co/allenai/OLMo-7B
